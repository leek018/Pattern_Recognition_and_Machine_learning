# 1장

+ Supervised learning 문제 : 주어진 훈련데이터가 입력벡터와 그에 해당하는 표적 벡터로 이루어지는 문제
  + Classification 문제 : 입력 벡터를 제한된 숫자의 분리된 카테고리 중 하나에 할당하는 종류의 지도 학습 문제
  + Regression 문제 : 기대되는 출력값이 하나 또는 그 이상의 연속된 값일 경우

+ Unsupervised learning 문제: 훈련 데이터가 해당 표적 벡터 없이 오직 입력 벡터 x로만 주어지는 경우의 패턴 인식 문제
  + Clustering 문제 : 데이터 내에서 비슷한 예시들의 집단을 찾는 문제
  + Densitiy estimation : 입력 공간에서의 데이터의 분포를 찾는 밀도 추정
+ Reinforcement learning : 주어진 상황에서 보상을 최대화하기 위한 행동을 찾는 문제

### 1-1 다항식 곡선 피팅

+ sin(2πx) 를 피팅하는 다항식 곡선을 예측해보자

+ 
  $$
  y(x,w) = w_0 + w_1x + w_2x^2 +...+w_Mx^M = \sum_{j=1}^Mx^j
  $$

+ 이때, 차수 **M**을 결정하는 것이 모델결정

+ 적절한 w를 찾기 위해 최소제곱법으로 오차 함수를 정의한다

+ $$
  E(w)=\frac{1}{2}\sum_{n=1}^N\{y(x_n,w)-t_n\}^2
  $$

  
  $$
  x_n은 데이터 포인트, N은 데이터의 개수
  $$

+ w에대해 미분하여 E(w)가 최소값을 갖는 w*을 찾는다. 
+ M이 0,1일 때보다 9일 때 오차가 작음 그러나 테스트에서 안좋은 성과를 보임. 이를 over-fitting이라고 함.
+ 일반적으로 모델의 복잡도가 증가할 수록 over-fitting되기 쉽고 모델의 복잡도가 일정하고 학습 데이터가 많으면 over-fitting문제가 완화됨.



+ 비교적 복잡하고 유연한 모델을 제한적인 숫자의 데이터 집합을 활용하여 피팅하기 위해서 regularization을 사용한다.

+ $$
  E(w)=\frac{1}{2}\sum_{n=1}^N\{y(x_n,w)-t_n\}^2 + \frac{λ}{2}\lVert w \rVert^2
  $$

  

+ 언뜻 보면 덧셈하는게 왜 Penaly를 주는거지 할 수 있지만 y(xn,w)를 일차 선형 모델이라 가정하고 위 식을 미분하여 w를 갱신하는 것을 풀어보면 이해가 된다